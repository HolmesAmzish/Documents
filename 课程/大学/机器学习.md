# K近邻算法（KNN）

## 算法思想

算法主要思路：如果一个样本在特征空间中k个实例最为相似，即特征空间中最邻近，那么这k个实例中大多数属于哪个类别，则该样本也属于这个类别。

### 算法流程

1. 计算测试对象到训练集中每个对象的距离；
2. 按照距离的远近排序；
3. 选取与当前测试对象最近的k的训练对象，作为该测试对象的邻居；
4. 统计这k个邻居的类别频次；
5. k个邻居中频次最高的类别，即为测试对象的类别。

### 关键要素

1. 距离度量：特征空间中样本点的距离是样本点间向此程度的反应。
2. 算法超参数k的取值
3. 决策规则：对于分类任务，采取少数服从多数；对于回归任务，采用平均值规则。

## 距离度量

距离度量是两个点之间相似程度的体现。

### 欧氏距离

欧氏距离是一个通常采用的距离定义，也是两个点在m维空间的真实距离，常见的有二维的平面距离和三维空间距离。
$$
d(x, y) = \sqrt{\sum_i{(x_i - y_i)^2}}
$$

### 曼哈顿距离

$$
d(x, y) = \sum_i|x_i - y_i|
$$

### 闵可夫斯基距离

$$
d(x, y) = (\sum_i|x_i - y_i|^p)^{\frac{1}{p}}
$$

p=1 时为曼哈顿距离，p=2 时为欧式距离。当p 取无限大时有切比雪夫距离。

### K取值



## Python 实现



# K均值算法（Kmeans）

# 决策树算法

# 贝叶斯分类

# 回归模型

## 一元线性回归

假设一元线性回归方程如下，需要寻找最佳参数w，b以拟合所有的样本数据。
$$
\hat{y} = wx + b
$$

### 性能度量

**SSE误差平方和（Sum of Squares due to Error）**：$$SSE = \sum_{i = 1}^n (y_i - \hat{y}_i)^2$$

**RMSE均方根误差（Root Mean Square Error）**：$$RMSE = \sqrt{\frac{1}{n}\sum_{i = 1}^n (y_i - \hat{y}_i)^2}$$

**MSE均方误差（Mean Square Error）**：$$MSE = \frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y}_i)^2$$

**MAE平均绝对误差（Mean Absolute Error）**：$$MAE = \frac{1}{n}\sum_{i = 1}^n |y_i - \hat{y}_i|$$​

### 最小二乘法



# 遗传算法

# 逻辑回归

# 神经网络

